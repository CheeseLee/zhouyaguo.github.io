---
layout: post
title: "[openstack] Nova源码分析"
comments: true
categories:
- openstack
---

_注： 代码基于nova-2012.1.2，其中的代码有一些bug，但不影响整体的分析过程。_

Controller Node 进程一览
========================

```
vm_156e3:~ # ps -ef|grep nova|grep -v grep
root      4878     1  0 Aug26 ?        00:00:43 /usr/bin/python /usr/share/noVNC/utils/nova-novncproxy --daemon --flagfile=/etc/nova/nova.conf --web /usr/share/noVNC
root      5500     1  0 Aug26 ?        01:33:53 /usr/bin/python /usr/bin/nova-api --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/api.log
root      5512     1  0 Aug26 ?        00:23:23 /usr/bin/python /usr/bin/nova-cert --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/cert.log
root      5530     1  0 Aug26 ?        00:52:31 /usr/bin/python /usr/bin/nova-compute --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/compute.log
root      5552     1  0 Aug26 ?        00:23:24 /usr/bin/python /usr/bin/nova-console --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/console.log
root      5568     1  0 Aug26 ?        00:23:59 /usr/bin/python /usr/bin/nova-consoleauth --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/consoleauth.log
root      5584     1  0 Aug26 ?        00:37:54 /usr/bin/python /usr/bin/nova-ha --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/ha.log
root      5607     1  0 Aug26 ?        00:32:33 /usr/bin/python /usr/bin/nova-network --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/network.log
root      5619     1  0 Aug26 ?        00:26:18 /usr/bin/python /usr/bin/nova-scheduler --flagfile=/etc/nova/nova.conf --logfile=/var/log/nova/scheduler.log
```


nova组成部分
==========

nova包括以下组件，他们共用配置文件：`/etc/nova/nova.conf`

- nova-api
- nova-compute
- nova-network
- nova-scheduler
- nova-console
- nova-consoleauth
- nova-cert
- nova-novncproxy

概述
====

- **nova-api** 的主要工作是：
	- 监听端口
	- 通过deployment类库从
	- 使用eventlet pool来接受http request
	- 通过python route将请求路由给特定的controller处理
	- 检查policy是否允许
	- 检查quota是否够用
	- 查看是否用户指定了某compute node
	- 从request中取出所有要素，通过rabbitmq发送给nova-scheduler
- **nova-scheduler** 的主要工作是：
	- 筛选出合适的compute node
	- 在表instaces中新建记录，并记录scheduled_at时间
	- 交给nova-compute处理
- **nova-compute** 的主要工作是：
	- 进行必要的检查，例如：uuid是否重复、image大小不能超过套餐大小
	- 更新数据库记录状态-BUIDING
	- 向nova-network索取ip-NETWORKING
	- 准备block mapping-BLOCK_DEVICE_MAPPING
	- spawn-SPAWNING
	- 具体spwan的工作是：
		- 生成最终要用的libvirt.xml
		- 从glance索取image文件，缓存在本地并拷贝出一个实例
		- mount该实例，进行inject操作，包括设root密码、网络配置、ssh key、inject files
		- 调用libvirt connection api来使用生成的libvirt.xml

详细
====

nova-api启动
------------

nova-api分为下面几项，每项都会listen独立的port、独立的url routes

- ec2
- osapi_compute
- osapi_volume
- metadata

启动入口： `/usr/bin/nova-api` ：

- 解析/etc/nova/nova.conf，保存到FLAGS中。
- 给logging设置日志目的地： /var/log/nova/api.log （启动命令行指定了）
- 根据FLAGS.enabled_apis，为每个api创建WSGIService对象，并通过eventlet来spawn出greenThread，用以启动WSGIService对象
- WSGIService的创建和启动过程（以osapi_compute为例）
	- 创建过程
		- WSGIService的构造函数
			- 设置name，即`FLAGS.enabled_apis`中的单个值
			- 设置manager，FLAGS.$name\_manager。（osapi_compute无manager，metadata有manager）
			- 新建Loader
				- 找`FLAGS.api_paste_config`，值为`api-paste.ini`
			- app=loader.load_app
				- 调用deploy.loadapp(osapi_compute) --详见paste deploy
			- 找`FLAGS.osapi_compute_listen`
			- 找`FLAGS.osapi_compute_listen_port`
			- 创建Server对象:构造函数
				- 传递进app
				- eventlet.GreenPool创建pool，default\_pool_size=1000
	- 启动过程
		- 如果有manager，则manager.init_host()
		- Server.start
			- eventlet.listen
			- eventlet.spawn调用_start
			- 调用`eventlet.wsgi.server(_socket, app, _protocol, _pool, _wsgi_logger)`启动。
				- 之后的请求都会达到app中处理

deploy.loadapp(osapi_compute)过程
-----------------------------------

- 从api-paste.ini中找到`[composite:osapi_compute]`
- 根据`use = call:nova.api.openstack.urlmap:urlmap_factory`，直接调用urlmap_factory方法
	- 分别loader.get\_app() （以openstack\_compute\_api_v2为例）
		- 直接调用 nova.api.auth:pipeline_factory （以keystone为例）
			- 直接调用osapi\_compute\_app_v2，再加上filters: faultwrap authtoken keystonecontext ratelimit
				- 直接调用nova.api.openstack.compute:APIRouter.factory
				- set routes （包括主要的服务和扩展服务）


nova-scheduler启动
------------------

启动入口： `/usr/bin/nova-scheduler` ：

- 解析/etc/nova/nova.conf，保存到FLAGS中。
- 给logging设置日志目的地： /var/log/nova/scheduler.log （启动命令行指定了）
- 创建Service
	- 设置topic为scheduler
	- 设置manager为FLAGS.get('%s\_manager' % topic)，即scheduler_manager
	- report_interval，默认10s
	- periodic_interval，默认60s
- 启动Service
	- eventlet.spawn
	- server.start -> Service.start
	- manager.init_host() 。
	- 查db，service表，如果没查到，则创建一条记录
	- 如果是nova-compute，则manager.update\_available_resource  -- 对于scheduler而言，没有
	- rpc.create_connection 创建queue connection
	- 为scheduler topic 创建consumer
	- 定时report_state -- update service表
	- 定时periodic_tasks  -- 对于scheduler而言，没有。

/usr/bin/nova-compute启动
-------------------------

启动入口： `/usr/bin/nova-compute` ：

- 解析/etc/nova/nova.conf，保存到FLAGS中。
- 给logging设置日志目的地： /var/log/nova/compute.log （启动命令行指定了）

- 创建Service
	- 设置topic为compute
	- 设置manager为FLAGS.get('%s\_manager' % topic)，即compute_manager: nova.compute.manager.ComputeManager
		- ComputeManager构造函数
			- 设置compute\_driver=FLAGS.compute\_driver，即nova.virt.connection.get_connection
			- 设置driver为`LibvirtConnection`，LibvirtConnection是ComputeDriver的子类
			- 设置network_api = network.API()
			- 设置volume_api = volume.API()
			- 设置network\_manager = utils.import\_object(FLAGS.network_manager)
	- report_interval，默认10s
	- periodic_interval，默认60s
- 启动Service
	- eventlet.spawn
	- server.start -> Service.start
	- ComputeManager.init_host()
	 	- LibvirtConnection.init_host() -> pass
		- 从db中查找host上所有的instances，查看每个instance的state，根据FLAGS.resume\_guests\_state\_on\_host\_boot，FLAGS.start\_guests\_on\_host_boot参数决定是否要重启
	- update\_available_resource
		- LibvirtConnection.update\_available_resource
			- 查compute_nodes、services表，更新很多字段。
	- 为compute topic 创建 consumer
	- 定时report_state -- update service表
	- 定时periodic_tasks

创建虚机的过程
-------------

```
user -> "nova-api"  : http request
"nova-api" -> server.Controller.create : route
server.Controller.create -> nova.compute.api.API.create : set params
nova.compute.api.API.create -> nova.compute.api.API.create : check policy
```

**nova.compute.api.API.create** 从http request中取出参数：

- flavor id，到db中查找对象的instance type记录
- image uuid
- access\_ip_v4（request中可能没指定）
- personality中的path，contents，作为injected_files
- root密码，如果password为空，则utils.generate\_password(FLAGS.password_length生成一个。
- min\_count，max_count
- networks，提取uuid，fixed_ip信息
- security_groups的名字，如果指定了，则再在后面追加上default安全组
- user_data
- availability_zone
- config_drive
- auto\_disk_config
- scheduler_hints

**nova.compute.api.API.create过程：**

- check\_create_policies （策略在policy.json中，如果不符合策略，则抛出NotAuthorized异常
- check quota （nova.conf中可以为每个project设置quota，包括core，ram，instances，inject_files等，如果超额，则QuotaError）
	- 计算allowed_instances（core，ram等）
	- check\_metadata\_properties_quota
	- check\_injected\_file_quota
	- check\_requested_networks
- 检查申请的memory是否比套餐小，否则抛InstanceTypeMemoryTooSmall (从FLAGS.glance\_api\_servers随机挑选一个，创建GlanceClient，放入GlanceImageService对象中)
- ensure\_default\_security_group 确保context至少有一个安全组
- 如果availability\_zone中：后面的host名字不为空，则将host设置到filter_properties的force_hosts中。
- 准备好base_options

	```
	base_options = {
	'reservation_id': reservation_id,
	'image_ref': image_href,
	'kernel_id': kernel_id or '',
	'ramdisk_id': ramdisk_id or '',
	'power_state': power_state.NOSTATE,
	'vm_state': vm_states.BUILDING,
	'config_drive_id': config_drive_id or '',
	'config_drive': config_drive or '',
	'user_id': context.user_id,
	'project_id': context.project_id,
	'launch_time': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
	'instance_type_id': instance_type['id'],
	'memory_mb': instance_type['memory_mb'],
	'vcpus': instance_type['vcpus'],
	'root_gb': instance_type['root_gb'],
	'ephemeral_gb': instance_type['ephemeral_gb'],
	'display_name': display_name,
	'display_description': display_description,
	'user_data': user_data or '',
	'key_name': key_name,
	'key_data': key_data,
	'locked': False,
	'metadata': metadata,
	'access_ip_v4': access_ip_v4,
	'access_ip_v6': access_ip_v6,
	'availability_zone': availability_zone,
	'os_type': os_type,
	'architecture': architecture,
	'vm_mode': vm_mode,
	'root_device_name': root_device_name,
	'progress': 0,
	'auto_disk_config': auto_disk_config}
	```
- 发送message到FLAGS.scheduler\_topic中，调用远程run_instance方法
- 由于nova-scheduler启动时，创建了FLAGS.scheduler\_topic的consumer，所以消息的处理位于Service的`__getattr__`
- SchedulerManager.run_instance
	- nova.scheduler.multi.MultiScheduler.schedule\_run_instance
		- nova.scheduler.filter\_scheduler.FilterScheduler.FilterScheduler.schedule\_run_instance
			- 取num_instances，看用户要创建几台
			- notifier.notify一下要开始了（“scheduler.run_instance.start”）
				- 调用FLAGS.notification_driver的notify方法(默认实现是nova.notifier.no_op_notifier，什么都不干)
			- 筛选出符合用户请求的机器
			  - populate filter_properties，填充filter条件。
				- host\_manager.get\_all\_host\_states 从db中找出所有HostManager知道的hosts dict(compute_node、service表)
				- host\_manager.filter_hosts开始用很多filter来筛选（其中就用到了force_hosts，即用户指定的host）
				- 筛选好后，将host consume resources，减去内存记录的host state。为了让后面的申请依据最新的场景。至于db中的compute_nodes表，会由libvirt去减。
				- 按weight将hosts排序
			- 每要新建一台，就调用weighted\_hosts.pop(0)一下，拿出一个host，调用provision\_resource开始做。
				- 在instances表里新建一个记录
				- scheduler.driver.cast\_to\_compute\_host("run_instance")
				- 更新instance表中记录的scheduled_at时间
				- 发送消息到queue中
				- 由于nova-compute启动时创建了consumer，所以消息的处理位于ComputeManager.run_instance（见下节）
			- notifier.notify通知一下scheduler做完了。

**nova-compute处理message过程**


即ComputeManager.run_instance：（这在compute node上执行）

- 确保instance uuid不重复
	- 调用compute_driver即nova.virt.connection.get_connection即libvirt connection，判断是否instance_exists
- 确保image的大小不要超过instance-type的大小
- 标记buiding状态
- notify一下
- 分配网络
	- 更新状态为NETWORKING
	- 调用network_api分配ip地址
- 准备block device
	- 更新状态为BLOCK\_DEVICE_MAPPING
	- 从db中查表：block\_device_mapping
	- TODO
- \_spawn
	- 更新状态为SPAWNING
	- 准备好admin密码和injected_files
	- driver.spawn(即LibvirtConnection.spawn，见下节)
	- 更新instances表（修改power\_state、vm\_state），其中power\_state的值来自LibvirtConnection.get\_info方法，取state值
- \_update\_access\_ip
- notify一下 create.end

**LibvirtConnection.spawn过程**

- to_xml
	- \_prepare\_xml_info （模板中的变量）
		- type
		- name
		- uuid
		- cachemode
		- basepath
		- memory_kb
		- vcpus
		- rescue
		- disk_prefix
		- driver_type  (qcow2 or raw)
		- root\_device\_type (cdrom or disk)
		- vif_type (bridge)
		- nics
		- ebs_root
		- ephemeral_device
		- volumes
		- use\_virtio\_for_bridges
		- ephemerals
		- root_device
		- root\_device_name
		- swap_device
		- config_drive
		- vncserver_listen
		- vnc_keymap
		- kernel
		- ramdisk
		- disk
	- 使用[Cheetah template](http://cheetahtemplate.org/index.html) 修改FLAGS.libvirt\_xml_template的内容，放内存里
- firewall_driver 设置策略
- \_create_image
  - ensure directories exist and are writable
	- 生成libvirt.xml
	- sha1来定义root_fname
	- \_cache_image
		- 调用libvirt\_utils.fetch\_image把远程image取到instance目录的_base目录下（image_service.get）
		- copy\_and_extend
			- 从_base目录cp到独立的vm目录去
			- 如果申请的size大于镜像模板大小，则调用`qemu-img resize`来修改大小，e2fsck，resize2fs来resize filesystem
		- inject\_data (ssh key, net, metadata, admin_password) (Injects a ssh key and optionally net data into a disk image)
		- inject_files (Injects arbitrary files into a disk image)
		- inject的方法就是将image mount到主机上，然后修改、拷贝。（nbd.Mount.do_mount） ,kpartx -a 可以查看image内部分区。注意：kpartx对raw格式无用
- 根据xml来create\_new_domain
	- 调用libvirt connect的defineXML方法

启动虚机的过程
------------

- http请求：

	```
	REQ: curl -i http://172.17.140.73:8774/v1.1/596ef07ab9a34ed1bf5f88c1db4de7bc/servers/064f37c7-c6b9-4dd9-aa33-87d25febe7aa/action -X POST -H "X-Auth-Project-Id: tenant" -H "User-Agent: python-novaclient" -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Auth-Token: 867c4317c1ec46a0b9bc370b92279911"

	REQ BODY: {"os-start": null}
	```

- server\_start\_stop的\_start_server方法

	```python
	@wsgi.action('os-start')
	def _start_server(self, req, id, body):
		"""Start an instance. """
		context = req.environ['nova.context']
		LOG.debug(_("start instance %r"), id)
		instance = self._get_instance(context, id)
		self.compute_api.start(context, instance)
		return webob.Response(status_int=202)
	```
- 根据uuid到db查出instance
- 调用compute API的start方法，在message中指定compute node

	```python
	@wrap_check_policy
    @check_instance_state(vm_state=[vm_states.STOPPED, vm_states.SHUTOFF])
    def start(self, context, instance):
        """Start an instance."""
        vm_state = instance["vm_state"]
        instance_uuid = instance["uuid"]
        LOG.debug(_("Going to try to start instance"), instance=instance)

        if vm_state == vm_states.SHUTOFF:
            if instance['shutdown_terminate']:
                LOG.warning(_("Instance %(instance_uuid)s is not "
                              "stopped. (%(vm_state)s") % locals())
                return

            # NOTE(yamahata): nova compute doesn't reap instances
            # which initiated shutdown itself. So reap it here.
            self.stop(context, instance, do_cast=False)

        self.update(context,
                    instance,
                    vm_state=vm_states.STOPPED,
                    task_state=task_states.STARTING)

        # TODO(yamahata): injected_files isn't supported right now.
        #                 It is used only for osapi. not for ec2 api.
        #                 availability_zone isn't used by run_instance.
        self._cast_compute_message('start_instance', context, instance)
	```
- 调用ComputeManager的start_instance方法
	- 该方法在nova-2012.1.2中bug，请看新版本的nova代码。


迁移虚机的过程
------------

- nova.api.openstack.compute.contrib.admin_actions.\_migrate\_live方法接受请求
	- check authorize
	- 从请求中取出三个参数
		- block_migration
		- disk\_over_commit
		- host
	- 调用compute.API.get方法，从instance表中查记录
	- 调用scheduler.api.live_migration
	- rpc.call 向scheduler\_topic中放消息，调用live_migration方法
	- Scheduler.schedule\_live\_migration
		- \_live_migration\_src\_check
			- 判断instance的power\_state必须是RUNNING或BLOCKED
			- 从compute_node表找出src host，判断是否up（判断updated\_at的时间和当前时间的差值是否在FLAGS.service\_down\_time时间内，默认60）
		- \_live_migration\_dest\_check
			- - 从compute_node表找出dest host，判断是否up（判断updated\_at的时间和当前时间的差值是否在FLAGS.service\_down\_time时间内，默认60）
			- 判断dest不能与src一样，即不能migrate到同一台机器
			- 检查dest是否有足够的资源：assert\_compute\_node\_has\_enough\_resources
				- 从compute_nodes查记录
				- 从instance表查node上的所有vm
				- 如果两者相减后的内存比instance的memory_mb的要少，则报错，内存不够，不能迁移
			- 如果是block\_migration，则assert\_compute\_node\_has\_enough\_disk
				- 从compute\_nodes表中查出available\_gb（disk\_available\_least字段）
				- rpc.call调compute节点get\_instance\_disk\_info
				- 计算差值，看是否满足disk的要求。
		- \_live_migration\_common\_check
			- check是否src, dest是否mount了同样的shared storage，采用的方法是在dest上创建一个temp file，看src上是否可以看到该文件
			- 检查hypervisor\_type是否是一样
			- 检查hypervisor\_version，dest不能低于src
			- rpc.call调用compute node的compare_cpu方法，检查cpuinfo，即检查dest主机的cpu是否和instance一致。
		- 修改instance表，设置vm_state为MIGRATING
		- cast\_to\_compute\_host
		- ComputeManager.live\_migration
			- 从db中查instance信息
			- rpc.call 调用compute的pre\_live_migration (准备工作)
				- 从block\_device\_mapping表中查，看instance是否有已mount的volume
				- libvirt.connection.pre\_live\_migration(block\_device\_info)
				- setup\_networks\_on\_host在dest host上配置网络
				- 获取instance的network信息
				- 调用libvirt.connection.plug_vifs
			- 调用libvirt.connection.live_migration
				- greenthread.spawn独立线程去做
				- conn.lookupByName
				- dom.migrateToURI，如果发生异常，则调用rollback\_live\_migration （见下）
				- loop等待，去获取instance的state，如果发生NotFound的异常，则调用post\_live\_migration （见下）
			- post\_live_migration
				- Detaching volumes
				- Releasing vlan
				- Database updating
				- 调用dest host的post\_live\_migration\_at\_destination
				- libvirt.connection.post\_live\_migration\_at\_destination，直接defineXML一下，启动
				- 修改instance的vm_state为ACTIVE
				- setup\_networks\_on_host去update dbcp
				- Restore volume state
				- unplug_vifs
			- rollback\_live_migration
				- Recovers Instance/volume state from migrating -> running
